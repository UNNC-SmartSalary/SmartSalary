{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- load the environment\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import codecs\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successful\n"
     ]
    }
   ],
   "source": [
    "#-- connect to the postgresql sever\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname = \"postgres\", user=\"readonly\", host= \"47.103.73.205\" ,password= \"123456\", port  = \"25432\")\n",
    "    print (\"connection successful\")\n",
    "    cur = conn.cursor()\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "    \n",
    "#-- selct the 10 items and then to pd.dataframe \n",
    "cur.execute(\"SELECT * FROM  meta_training_data_jd WHERE fun1_code LIKE 'ALG' or fun1_code LIKE 'BSC' ;\")\n",
    "rows=cur.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>203341</td>\n",
       "      <td>AppleOne</td>\n",
       "      <td>Legal Secretary  -</td>\n",
       "      <td>\\n\\nJob Description:This Legal Secretary  Posi...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>212660</td>\n",
       "      <td>Staffing Now</td>\n",
       "      <td>Litigation Secretary</td>\n",
       "      <td>\\n\\nSNI Companies has partnered with one of th...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>267508</td>\n",
       "      <td>Beacon Hill Staffing Group, LLC</td>\n",
       "      <td>Legal Secretary</td>\n",
       "      <td>\\n\\nWe are currently in need of Legal Secretar...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>267534</td>\n",
       "      <td>People People</td>\n",
       "      <td>Litigation Secretary</td>\n",
       "      <td>\\n\\nMcAnany, Van Cleave and Phillips (MVP) is ...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>267672</td>\n",
       "      <td>Kelly Services</td>\n",
       "      <td>Legal Secretary / Legal Assistant</td>\n",
       "      <td>\\n\\nJob DescriptionApply Now!Open Legal Secret...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2424</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG040</td>\n",
       "      <td>ALG</td>\n",
       "      <td>040</td>\n",
       "      <td>诉讼</td>\n",
       "      <td>Litigation</td>\n",
       "      <td>457667</td>\n",
       "      <td>Cohen Compagni Beckman Appler &amp; Knoll, PLLC</td>\n",
       "      <td>Legal Assistant - Litigation</td>\n",
       "      <td>\\n\\nJob Purpose:Serves clients by managing cas...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>176944</td>\n",
       "      <td>Legal Network Inc</td>\n",
       "      <td>Legal Secretary</td>\n",
       "      <td>\\n\\nTERRIFIC LEGAL SECRETARY POSITION AVAILABL...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2426</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>179380</td>\n",
       "      <td>Montana State Government</td>\n",
       "      <td>Administrative Support-Legal Secretary-Part-Time</td>\n",
       "      <td>\\n\\nDescription:To be considered for OPD posit...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2427</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>201104</td>\n",
       "      <td>Cummings &amp; Lockwood LLC</td>\n",
       "      <td>Legal Secretary</td>\n",
       "      <td>\\n\\nLegal SecretaryCummings &amp; Lockwood LLC Sta...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2428</td>\n",
       "      <td>DIS</td>\n",
       "      <td>ALG004</td>\n",
       "      <td>ALG</td>\n",
       "      <td>004</td>\n",
       "      <td>企业/公司秘书（担负法务职责）</td>\n",
       "      <td>Corporate/Company Secretary (with Legal Respon...</td>\n",
       "      <td>201265</td>\n",
       "      <td>Sole Law, PLLC</td>\n",
       "      <td>Legal Secretary/Assistant</td>\n",
       "      <td>\\n\\nGrowing downtown real estate and business ...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2429 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1    2    3                4  \\\n",
       "0     DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "1     DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "2     DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "3     DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "4     DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "...   ...     ...  ...  ...              ...   \n",
       "2424  DIS  ALG040  ALG  040               诉讼   \n",
       "2425  DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "2426  DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "2427  DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "2428  DIS  ALG004  ALG  004  企业/公司秘书（担负法务职责）   \n",
       "\n",
       "                                                      5       6  \\\n",
       "0     Corporate/Company Secretary (with Legal Respon...  203341   \n",
       "1     Corporate/Company Secretary (with Legal Respon...  212660   \n",
       "2     Corporate/Company Secretary (with Legal Respon...  267508   \n",
       "3     Corporate/Company Secretary (with Legal Respon...  267534   \n",
       "4     Corporate/Company Secretary (with Legal Respon...  267672   \n",
       "...                                                 ...     ...   \n",
       "2424                                         Litigation  457667   \n",
       "2425  Corporate/Company Secretary (with Legal Respon...  176944   \n",
       "2426  Corporate/Company Secretary (with Legal Respon...  179380   \n",
       "2427  Corporate/Company Secretary (with Legal Respon...  201104   \n",
       "2428  Corporate/Company Secretary (with Legal Respon...  201265   \n",
       "\n",
       "                                                7  \\\n",
       "0                                        AppleOne   \n",
       "1                                    Staffing Now   \n",
       "2                 Beacon Hill Staffing Group, LLC   \n",
       "3                                   People People   \n",
       "4                                  Kelly Services   \n",
       "...                                           ...   \n",
       "2424  Cohen Compagni Beckman Appler & Knoll, PLLC   \n",
       "2425                            Legal Network Inc   \n",
       "2426                     Montana State Government   \n",
       "2427                      Cummings & Lockwood LLC   \n",
       "2428                               Sole Law, PLLC   \n",
       "\n",
       "                                                     8  \\\n",
       "0                                   Legal Secretary  -   \n",
       "1                                 Litigation Secretary   \n",
       "2                                      Legal Secretary   \n",
       "3                                 Litigation Secretary   \n",
       "4                    Legal Secretary / Legal Assistant   \n",
       "...                                                ...   \n",
       "2424                      Legal Assistant - Litigation   \n",
       "2425                                   Legal Secretary   \n",
       "2426  Administrative Support-Legal Secretary-Part-Time   \n",
       "2427                                   Legal Secretary   \n",
       "2428                         Legal Secretary/Assistant   \n",
       "\n",
       "                                                      9       10  \n",
       "0     \\n\\nJob Description:This Legal Secretary  Posi...  monster  \n",
       "1     \\n\\nSNI Companies has partnered with one of th...  monster  \n",
       "2     \\n\\nWe are currently in need of Legal Secretar...  monster  \n",
       "3     \\n\\nMcAnany, Van Cleave and Phillips (MVP) is ...  monster  \n",
       "4     \\n\\nJob DescriptionApply Now!Open Legal Secret...  monster  \n",
       "...                                                 ...      ...  \n",
       "2424  \\n\\nJob Purpose:Serves clients by managing cas...  monster  \n",
       "2425  \\n\\nTERRIFIC LEGAL SECRETARY POSITION AVAILABL...  monster  \n",
       "2426  \\n\\nDescription:To be considered for OPD posit...  monster  \n",
       "2427  \\n\\nLegal SecretaryCummings & Lockwood LLC Sta...  monster  \n",
       "2428  \\n\\nGrowing downtown real estate and business ...  monster  \n",
       "\n",
       "[2429 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'t\",\n",
       " \"'ve\",\n",
       " 'ZT',\n",
       " 'ZZ',\n",
       " 'a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abst',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'adj',\n",
       " 'adopted',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ah',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'announce',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'approximately',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arent',\n",
       " 'arise',\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'auth',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'back',\n",
       " 'backed',\n",
       " 'backing',\n",
       " 'backs',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'beginnings',\n",
       " 'begins',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'beings',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'big',\n",
       " 'biol',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'briefly',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'ca',\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'couldnt',\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'date',\n",
       " 'definitely',\n",
       " 'describe',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'differ',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'discuss',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downed',\n",
       " 'downing',\n",
       " 'downs',\n",
       " 'downwards',\n",
       " 'due',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'early',\n",
       " 'ed',\n",
       " 'edu',\n",
       " 'effect',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'eighty',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'ends',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'et-al',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evenly',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'face',\n",
       " 'faces',\n",
       " 'fact',\n",
       " 'facts',\n",
       " 'far',\n",
       " 'felt',\n",
       " 'few',\n",
       " 'ff',\n",
       " 'fifth',\n",
       " 'find',\n",
       " 'finds',\n",
       " 'first',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'further',\n",
       " 'furthered',\n",
       " 'furthering',\n",
       " 'furthermore',\n",
       " 'furthers',\n",
       " 'g',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'goods',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatest',\n",
       " 'greetings',\n",
       " 'group',\n",
       " 'grouped',\n",
       " 'grouping',\n",
       " 'groups',\n",
       " 'h',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he's\",\n",
       " 'hed',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'heres',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hes',\n",
       " 'hi',\n",
       " 'hid',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highest',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'home',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'id',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'im',\n",
       " 'immediate',\n",
       " 'immediately',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'include',\n",
       " 'indeed',\n",
       " 'index',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'information',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'interest',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interests',\n",
       " 'into',\n",
       " 'invention',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'itd',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'keys',\n",
       " 'kg',\n",
       " 'kind',\n",
       " 'km',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'l',\n",
       " 'large',\n",
       " 'largely',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'lets',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'line',\n",
       " 'little',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'longest',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'made',\n",
       " 'mainly',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meantime',\n",
       " 'meanwhile',\n",
       " 'member',\n",
       " 'members',\n",
       " 'men',\n",
       " 'merely',\n",
       " 'mg',\n",
       " 'might',\n",
       " 'million',\n",
       " 'miss',\n",
       " 'ml',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'mug',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " \"n't\",\n",
       " 'na',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nay',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessarily',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needing',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'newer',\n",
       " 'newest',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'ninety',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'nonetheless',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'nos',\n",
       " 'not',\n",
       " 'noted',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'o',\n",
       " 'obtain',\n",
       " 'obtained',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'older',\n",
       " 'oldest',\n",
       " 'omitted',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'opening',\n",
       " 'opens',\n",
       " 'or',\n",
       " 'ord',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'ordering',\n",
       " 'orders',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'owing',\n",
       " 'own',\n",
       " 'p',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'part',\n",
       " 'parted',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'parting',\n",
       " 'parts',\n",
       " 'past',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'place',\n",
       " 'placed',\n",
       " 'places',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'pointed',\n",
       " 'pointing',\n",
       " 'points',\n",
       " 'poorly',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'potentially',\n",
       " 'pp',\n",
       " 'predominantly',\n",
       " 'present',\n",
       " 'presented',\n",
       " 'presenting',\n",
       " 'presents',\n",
       " 'presumably',\n",
       " 'previously',\n",
       " 'primarily',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'promptly',\n",
       " 'proud',\n",
       " 'provides',\n",
       " 'put',\n",
       " 'puts',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quickly',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'ran',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'readily',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'ref',\n",
       " 'refs',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'related',\n",
       " 'relatively',\n",
       " 'research',\n",
       " 'respectively',\n",
       " 'resulted',\n",
       " 'resulting',\n",
       " 'results',\n",
       " 'right',\n",
       " 'room',\n",
       " 'rooms',\n",
       " 'run',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'sec',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'seconds',\n",
       " 'section',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'sees',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " \"she'll\",\n",
       " 'shed',\n",
       " 'shes',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'show',\n",
       " 'showed',\n",
       " 'showing',\n",
       " 'shown',\n",
       " 'showns',\n",
       " 'shows',\n",
       " 'side',\n",
       " 'sides',\n",
       " 'significant',\n",
       " 'significantly',\n",
       " 'similar',\n",
       " 'similarly',\n",
       " 'since',\n",
       " 'six',\n",
       " 'slightly',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'smallest',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'somethan',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specifically',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'state',\n",
       " 'states',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'strongly',\n",
       " 'sub',\n",
       " 'substantially',\n",
       " 'successfully',\n",
       " 'such',\n",
       " 'sufficiently',\n",
       " 'suggest',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " \"t's\",\n",
       " 'take',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " \"that's\",\n",
       " \"that've\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " \"there'll\",\n",
       " \"there's\",\n",
       " \"there've\",\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'thered',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereof',\n",
       " 'therere',\n",
       " 'theres',\n",
       " 'thereto',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'theyd',\n",
       " 'theyre',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinks',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'though',\n",
       " 'thoughh',\n",
       " 'thought',\n",
       " 'thoughts',\n",
       " 'thousand',\n",
       " 'three',\n",
       " 'throug',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'til',\n",
       " 'tip',\n",
       " 'to',\n",
       " 'today',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'ts',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'turning',\n",
       " 'turns',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'u',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlike',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'ups',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'usefully',\n",
       " 'usefulness',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'uucp',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vol',\n",
       " 'vols',\n",
       " 'vs',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wanting',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'ways',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'wed',\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'wells',\n",
       " 'went',\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what'll\",\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'wheres',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whim',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who'll\",\n",
       " \"who's\",\n",
       " 'whod',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whomever',\n",
       " 'whos',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'widely',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " \"won't\",\n",
       " 'wonder',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'www',\n",
       " 'x',\n",
       " 'y',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'young',\n",
       " 'younger',\n",
       " 'youngest',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'z',\n",
       " 'zero',\n",
       " 'zt',\n",
       " 'zz']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = codecs.open(\"./stop_words.txt\").read().splitlines()\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The “root” in this case may not be a real root word, but just a canonical form of the original word.\n",
    "\n",
    "Stemming uses a crude heuristic process that chops off the ends of words in the hope of correctly transforming words into its root form. So the words “trouble”, “troubled” and “troubles” might actually be converted to troubl instead of trouble because the ends were just chopped off (ughh, how crude!).\n",
    "\n",
    "There are different algorithms for stemming. The most common algorithm, which is also known to be empirically effective for English, is Porters Algorithm. Here is an example of stemming in action with Porter Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "punctuation = '-!,;:?.-@()+&·*–…$\"\\''\n",
    "porter_stemmer=PorterStemmer()\n",
    "def removePunctuation(text):\n",
    "    text = re.sub(r'[{}]+'.format(punctuation),'',text)\n",
    "    text =  text.strip().lower()\n",
    "\n",
    "\n",
    "    words=re.split(\"\\\\s+\",text)\n",
    "    stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    " \n",
    "text = df.iloc[0,-2]\n",
    "# print(removePunctuation(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'job descriptionthi legal secretari posit featuresawesom opportun for experienc legal secretari with year of experi to join small fun famili orient firm wear mani hat from order offic suppli type contract and legal document prepar present keep attorney schedul interact with client qualifi candid should have year of legal experi with strong type skill attent to detail strong work knowledg of ms offic fun outlook on life and daili work compani valu worklif balanc not in center citi free park on site we are an equal employ opportun employ commit to excel divers and inclus you can view all of our job onlin at httpwwwappleonecomscid'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "for row in rows:\n",
    "    docs.append(removePunctuation(row[-2]))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = []\n",
    "for row in rows:\n",
    "    id.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carl\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'al', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2429, 15830)\n",
      "\n",
      "feature extraction done in 0.907478s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "count = TfidfVectorizer(max_df=0.5, min_df=2,  stop_words=stopwords)\n",
    "transform=TfidfTransformer()\n",
    "X=count.fit_transform(docs)\n",
    "X=transform.fit_transform(X)\n",
    "print(X.shape)\n",
    "print()\n",
    "feature=count.get_feature_names()\n",
    "\n",
    "\n",
    "print(\"feature extraction done in %fs\" % (time.time() - t0))\n",
    "print()\n",
    "\n",
    "### 将obj对象序列化存入已经打开的file中\n",
    "f1=open('filter_data.p','wb')\n",
    "pickle.dump(X,f1)\n",
    "f2=open('feature_5000.p','wb')\n",
    "pickle.dump(feature,f2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_matrix(text, label):\n",
    "    \"\"\"Input: ('text', label: list or array-like), generate a feature matrix with word count and label)\"\"\"\n",
    "    cv = CountVectorizer(analyzer='word', max_df=0.5, min_df=2,stop_words= stopwords)\n",
    "    cv_fit = cv.fit_transform(text)\n",
    "\n",
    "    feature_matrix = pd.DataFrame(cv_fit.toarray(), columns=cv.get_feature_names())\n",
    "    feature_matrix.insert(0, \"data_label\", label)\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = generate_feature_matrix(docs, id)\n",
    "# feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_matrix.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- X and y\n",
    "\n",
    "X = feature_matrix.drop(columns = 'data_label')\n",
    "y = feature_matrix.data_label\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=10, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth = 10, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. 0.5386533665835411 Recall: 0.5386533665835411 Specificity: 0.5386533665835411 Balanced: 0.3741470192512721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carl\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1317: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score,recall_score\n",
    "\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "balanced = balanced_accuracy_score(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred,average='micro')\n",
    "specificity = recall_score(y_test,y_pred, pos_label = 0,average='micro')\n",
    "\n",
    "print('Acc. {} Recall: {} Specificity: {} Balanced: {}'.format(acc,recall,specificity,balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.22803\tvalidation_1-merror:0.36658\n",
      "[1]\tvalidation_0-merror:0.16964\tvalidation_1-merror:0.34040\n"
     ]
    }
   ],
   "source": [
    "param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
    "\n",
    "clf = XGBClassifier(**param_dist)\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        eval_metric=\"merror\",\n",
    "        verbose=True,)\n",
    "\n",
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. 0.6596009975062345 Recall: 0.6596009975062345 Specificity: 0.6596009975062345 Balanced: 0.5760635093064937\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "balanced = balanced_accuracy_score(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred,average='micro')\n",
    "specificity = recall_score(y_test,y_pred, pos_label = 0,average='micro')\n",
    "\n",
    "print('Acc. {} Recall: {} Specificity: {} Balanced: {}'.format(acc,recall,specificity,balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPool1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-4989baaad031>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mlabel_onehot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mtrain_proportion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-4989baaad031>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mlabel_onehot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mtrain_proportion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "## define hyperparameters \n",
    "## Using TensorFlow backend.\n",
    "\n",
    "max_word=500 \n",
    "max_loop = 1000 \n",
    "vocab_size=4000 \n",
    "batch_size = 32 \n",
    "category_size=15830\n",
    "\n",
    "\n",
    "## prepare the data \n",
    "##with open('./filter_data.p', 'rb') as f: \n",
    "#    doc_vector=pickle.load(f)\n",
    "    \n",
    "##with open('./feature_5000.p','rb') as f: \n",
    "#    label=pickle.load(f)\n",
    "\n",
    "doc_vector = X\n",
    "label = feature\n",
    "\n",
    "aux=np.eye(category_size) \n",
    "label_onehot=np.array([aux[y] for y in label])\n",
    "\n",
    "random.seed = 100 \n",
    "train_proportion=0.7 \n",
    "max_length=1497\n",
    "\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(doc_vector)\n",
    "np.random.set_state(state) \n",
    "np.random.shuffle(label_onehot)\n",
    "\n",
    "\n",
    "total_size = len(label) \n",
    "cut = int(np.ceil(total_size * train_proportion))\n",
    "x_train = doc_vector[:cut] \n",
    "x_train=sequence.pad_sequences(x_train,maxlen=max_length) \n",
    "y_train = label_onehot[:cut]\n",
    "x_test = doc_vector[cut:] \n",
    "x_test=sequence.pad_sequences(x_test,maxlen=max_length) \n",
    "y_test = label_onehot[cut:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__</th>\n",
       "      <th>_busi</th>\n",
       "      <th>_close</th>\n",
       "      <th>_directorate_</th>\n",
       "      <th>_feb</th>\n",
       "      <th>_financeaccountingauditing_</th>\n",
       "      <th>_group_</th>\n",
       "      <th>_job</th>\n",
       "      <th>_mo</th>\n",
       "      <th>_open</th>\n",
       "      <th>...</th>\n",
       "      <th>zeh</th>\n",
       "      <th>zerohungerzerowast</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zion</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zr</th>\n",
       "      <th>zuora</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zycu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2427</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2429 rows × 15830 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      __  _busi  _close  _directorate_  _feb  _financeaccountingauditing_  \\\n",
       "0      0      0       0              0     0                            0   \n",
       "1      0      0       0              0     0                            0   \n",
       "2      0      0       0              0     0                            0   \n",
       "3      0      0       0              0     0                            0   \n",
       "4      0      0       0              0     0                            0   \n",
       "...   ..    ...     ...            ...   ...                          ...   \n",
       "2424   0      0       0              0     0                            0   \n",
       "2425   0      0       0              0     0                            0   \n",
       "2426   0      0       0              0     0                            0   \n",
       "2427   0      0       0              0     0                            0   \n",
       "2428   0      0       0              0     0                            0   \n",
       "\n",
       "      _group_  _job  _mo  _open  ...  zeh  zerohungerzerowast  zillow  zion  \\\n",
       "0           0     0    0      0  ...    0                   0       0     0   \n",
       "1           0     0    0      0  ...    0                   0       0     0   \n",
       "2           0     0    0      0  ...    0                   0       0     0   \n",
       "3           0     0    0      0  ...    0                   0       0     0   \n",
       "4           0     0    0      0  ...    0                   0       0     0   \n",
       "...       ...   ...  ...    ...  ...  ...                 ...     ...   ...   \n",
       "2424        0     0    0      0  ...    0                   0       0     0   \n",
       "2425        0     0    0      0  ...    0                   0       0     0   \n",
       "2426        0     0    0      0  ...    0                   0       0     0   \n",
       "2427        0     0    0      0  ...    0                   0       0     0   \n",
       "2428        0     0    0      0  ...    0                   0       0     0   \n",
       "\n",
       "      zip  zone  zr  zuora  zurich  zycu  \n",
       "0       0     0   0      0       0     0  \n",
       "1       0     0   0      0       0     0  \n",
       "2       0     0   0      0       0     0  \n",
       "3       0     0   0      0       0     0  \n",
       "4       0     0   0      0       0     0  \n",
       "...   ...   ...  ..    ...     ...   ...  \n",
       "2424    0     0   0      0       0     0  \n",
       "2425    0     0   0      0       0     0  \n",
       "2426    0     0   0      0       0     0  \n",
       "2427    0     0   0      0       0     0  \n",
       "2428    0     0   0      0       0     0  \n",
       "\n",
       "[2429 rows x 15830 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('split_data.p','wb') as f: pickle.dump((x_train,y_train,x_test,y_test),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## build the model \n",
    "model=Sequential() \n",
    "model.add(Embedding(vocab_size,64,input_length=max_length)) \n",
    "model.add(Dropout(0.2))\n",
    "# we add a Convolution1D, which will learn filters \n",
    "# word group filters of size filter_length: \n",
    "model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1)) \n",
    "# we use max pooling: \n",
    "model.add(GlobalMaxPooling1D())\n",
    "# We add a vanilla hidden layer: \n",
    "model.add(Dense(32)) \n",
    "model.add(Dropout(0.2)) model.add(Activation('relu'))\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid: \n",
    "model.add(Dense(category_size,activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print('the model is loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test,y_test),epochs=20,batch_size=64) \n",
    "scores=model.evaluate(x_test,y_test,verbose=1) \n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
