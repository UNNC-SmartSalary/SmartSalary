{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: count the combined words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:14:52.345141Z",
     "start_time": "2020-04-06T07:14:48.360670Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:14:57.609945Z",
     "start_time": "2020-04-06T07:14:55.933393Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Code</th>\n",
       "      <th>fun1_code</th>\n",
       "      <th>fun2_code</th>\n",
       "      <th>jobname_ch</th>\n",
       "      <th>jobname_en</th>\n",
       "      <th>id</th>\n",
       "      <th>coname</th>\n",
       "      <th>poname</th>\n",
       "      <th>jd</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FUN</td>\n",
       "      <td>BRO</td>\n",
       "      <td>BRO</td>\n",
       "      <td>BRO</td>\n",
       "      <td>商业研究</td>\n",
       "      <td>Business Research</td>\n",
       "      <td>279011</td>\n",
       "      <td>Aldevron</td>\n",
       "      <td>Strategic Account Manager, Nucleic Acids Busin...</td>\n",
       "      <td>\\n\\nWe are currently seeking a Strategic Accou...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUN</td>\n",
       "      <td>BRO</td>\n",
       "      <td>BRO</td>\n",
       "      <td>BRO</td>\n",
       "      <td>商业研究</td>\n",
       "      <td>Business Research</td>\n",
       "      <td>287601</td>\n",
       "      <td>Synergy America</td>\n",
       "      <td>IT Business Analyst</td>\n",
       "      <td>\\n\\nWe are looking for an IT Business Analyst ...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FUN</td>\n",
       "      <td>BRO</td>\n",
       "      <td>BRO</td>\n",
       "      <td>BRO</td>\n",
       "      <td>商业研究</td>\n",
       "      <td>Business Research</td>\n",
       "      <td>340361</td>\n",
       "      <td>A3 Smart Home</td>\n",
       "      <td>Sr. Business Analyst</td>\n",
       "      <td>\\n\\nWe call our club's vision, mission, values...</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type Code fun1_code fun2_code jobname_ch         jobname_en      id  \\\n",
       "0  FUN  BRO       BRO       BRO       商业研究  Business Research  279011   \n",
       "1  FUN  BRO       BRO       BRO       商业研究  Business Research  287601   \n",
       "2  FUN  BRO       BRO       BRO       商业研究  Business Research  340361   \n",
       "\n",
       "            coname                                             poname  \\\n",
       "0         Aldevron  Strategic Account Manager, Nucleic Acids Busin...   \n",
       "1  Synergy America                                IT Business Analyst   \n",
       "2    A3 Smart Home                               Sr. Business Analyst   \n",
       "\n",
       "                                                  jd   source  \n",
       "0  \\n\\nWe are currently seeking a Strategic Accou...  monster  \n",
       "1  \\n\\nWe are looking for an IT Business Analyst ...  monster  \n",
       "2  \\n\\nWe call our club's vision, mission, values...  monster  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from local pathway\n",
    "\n",
    "project_path = \"/Users/apple/Documents/Xinzhi/Data/\"\n",
    "data = pd.read_csv(project_path + \"meta_training_data_jd_0401.csv\")\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a col of job title description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:15:00.411978Z",
     "start_time": "2020-04-06T07:15:00.406623Z"
    }
   },
   "outputs": [],
   "source": [
    "# get stop words\n",
    "\n",
    "def get_stopwords(words_file):\n",
    "    stopwords = []\n",
    "    with open(words_file, 'r') as file:\n",
    "        for word in file.readlines():\n",
    "            stopwords.append(word.strip('\\n'))\n",
    "    \n",
    "    return stopwords\n",
    "\n",
    "\n",
    "stopwords = get_stopwords(\"stop_words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:15:09.596271Z",
     "start_time": "2020-04-06T07:15:09.588550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tier 1 job title:  97\n",
      "The number of tier 2 job title:  709\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of tier 1 job title: \", len(data.fun1_code.unique()))\n",
    "print(\"The number of tier 2 job title: \", len(data.Code.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:15:10.881227Z",
     "start_time": "2020-04-06T07:15:10.828235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>fun1_code</th>\n",
       "      <th>fun2_code</th>\n",
       "      <th>jobname_ch</th>\n",
       "      <th>jobname_en</th>\n",
       "      <th>id</th>\n",
       "      <th>coname</th>\n",
       "      <th>poname</th>\n",
       "      <th>jd</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AFB080</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFY660</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AHR165</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMS900</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APM080</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APM900</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVO060</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BNV</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YHF020</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Type  fun1_code  fun2_code  jobname_ch  jobname_en  id  coname  \\\n",
       "Code                                                                     \n",
       "AFB080     1          1          1           1           1   1       1   \n",
       "AFY660     1          1          1           1           1   1       1   \n",
       "AHR165     1          1          1           1           1   1       1   \n",
       "AMS900     1          1          1           1           1   1       1   \n",
       "APM080     1          1          1           1           1   1       1   \n",
       "APM900     1          1          1           1           1   1       1   \n",
       "AVO060     1          1          1           1           1   1       1   \n",
       "BNV        1          1          1           1           1   1       1   \n",
       "YHF020     1          1          1           1           1   1       1   \n",
       "\n",
       "        poname  jd  source  \n",
       "Code                        \n",
       "AFB080       1   1       1  \n",
       "AFY660       1   1       1  \n",
       "AHR165       1   1       1  \n",
       "AMS900       1   1       1  \n",
       "APM080       1   1       1  \n",
       "APM900       1   1       1  \n",
       "AVO060       1   1       1  \n",
       "BNV          1   1       1  \n",
       "YHF020       1   1       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class 'Code' that only have 1 instance\n",
    "jd_count = data.groupby('Code').count()\n",
    "jd_count[jd_count['jd']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:15:48.423605Z",
     "start_time": "2020-04-06T07:15:48.379954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Code</th>\n",
       "      <th>fun2_code</th>\n",
       "      <th>jobname_ch</th>\n",
       "      <th>jobname_en</th>\n",
       "      <th>id</th>\n",
       "      <th>coname</th>\n",
       "      <th>poname</th>\n",
       "      <th>jd</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun1_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Type, Code, fun2_code, jobname_ch, jobname_en, id, coname, poname, jd, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class 'fun1_code' that only have 1 instance\n",
    "jd_count = data.groupby('fun1_code').count()\n",
    "jd_count[jd_count['jd']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 -  JD List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:16:04.704792Z",
     "start_time": "2020-04-06T07:16:04.699551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the list of jds\n",
    "jd_list = data.jd.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:16:24.760500Z",
     "start_time": "2020-04-06T07:16:24.757496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test base: `text_to_word_sequence`\n",
    "def segment_text_list(text_list):\n",
    "    text_list_segmented = [text_to_word_sequence(text) for text in text_list]\n",
    "    \n",
    "    return text_list_segmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Correct misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:16:44.929313Z",
     "start_time": "2020-04-06T07:16:44.924614Z"
    }
   },
   "outputs": [],
   "source": [
    "class CheckSpell():\n",
    "    def __init__(self):\n",
    "        self.spell = SpellChecker()\n",
    "    \n",
    "    def find_unknown(self, text_list_segmented):\n",
    "        unkonwn_words = [list(self.spell.unknown(word_list)) for word_list in text_list_segmented]\n",
    "        return unkonwn_words\n",
    "    \n",
    "    def spell_correction(self, text_list_segmented):\n",
    "        corrected = [[self.spell.correction(word) for word in word_list] for word_list in text_list_segmented]\n",
    "        return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Keep pure words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.658729Z",
     "start_time": "2020-04-05T12:28:05.652978Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# V1: clean the segmented text list, keep only pure english words        \n",
    "\n",
    "# def keep_pure_text(text_list_segmented):\n",
    "#     for word_list in text_list_segmented:\n",
    "#         for word in word_list:\n",
    "#             match = re.findall(r'^[a-z]+$', word)\n",
    "#             if match:\n",
    "#                 pass\n",
    "#             else: \n",
    "#                 word_list.remove(word)\n",
    "    \n",
    "#     return text_list_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T08:23:00.285118Z",
     "start_time": "2020-04-06T08:23:00.280873Z"
    }
   },
   "outputs": [],
   "source": [
    "# V2: clean the segmented text list, keep only pure english words        \n",
    "\n",
    "# def keep_pure(word_list):\n",
    "#     for word in word_list:\n",
    "#         match = re.findall(r'^[a-z]+$', word)\n",
    "#         if match:\n",
    "#             pass\n",
    "#         else: \n",
    "#             word_list.remove(word)\n",
    "\n",
    "\n",
    "# def keep_pure_text(text_list_segmented):\n",
    "#     result = [keep_pure(word_list) for word_list in text_list_segmented]\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T08:32:04.852598Z",
     "start_time": "2020-04-06T08:32:04.848225Z"
    }
   },
   "outputs": [],
   "source": [
    "# V3:clean the segmented text list, keep english words   \n",
    "\n",
    "def clean_word(word):\n",
    "    pattern=re.compile(r'[a-z][0-9]+') # check include dash -\n",
    "    match = pattern.findall(word)\n",
    "    new_word=' '.join(match)\n",
    "    \n",
    "    return new_word\n",
    "\n",
    "def keep_pure_text(text_list_segmented):\n",
    "    result = [[clean_word(word) for word in word_list] for word_list in text_list_segmented]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Filter Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:21:42.936780Z",
     "start_time": "2020-04-06T07:21:42.932455Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_stopwords(text_list_segmented, stop_words):\n",
    "    filtered_words = [[word for word in word_list if word not in stop_words]\n",
    "                      for word_list in text_list_segmented]\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:22:58.803369Z",
     "start_time": "2020-04-06T07:22:58.799551Z"
    }
   },
   "outputs": [],
   "source": [
    "def stemer_starter(text_list_segmented, in_stemmer='Porter'):\n",
    "    if in_stemmer == 'Snowball':\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "    \n",
    "    result = [[stemmer.stem(word) for word in word_list] for word_list in text_list_segmented]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:23:19.827650Z",
     "start_time": "2020-04-06T07:23:19.820852Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_list(word_list):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for word, tag in nltk.pos_tag(word_list):\n",
    "        if tag.startswith('NN'):\n",
    "            result.append(wnl.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB'):\n",
    "            result.append(wnl.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('JJ'):\n",
    "            result.append(wnl.lemmatize(word, pos='a'))\n",
    "        elif tag.startswith('R'):\n",
    "            result.append(wnl.lemmatize(word, pos='r'))\n",
    "        else:\n",
    "            result.append(word)\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def lemmatize_text_list(text_list_segmented):\n",
    "    result = [lemmatize_list(word_list) for word_list in text_list_segmented]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T10:03:58.169035Z",
     "start_time": "2020-04-06T10:03:58.165609Z"
    }
   },
   "outputs": [],
   "source": [
    "t26 = [['caresses', 'care', 'fly', 'flies', 'die', 'dies', 'died', 'mules', 'deny', 'denied'], \n",
    "       ['agree', 'agreed', 'own', 'owned', 'tradition', 'traditional', 'sensation', 'sensational'],\n",
    "       ['meet', 'meeting', 'plot', 'plotted', 'reference', 'references']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T10:04:18.901216Z",
     "start_time": "2020-04-06T10:04:18.898172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['caresses', 'care', 'fly', 'flies', 'die', 'dies', 'died', 'mules', 'deny', 'denied'], ['agree', 'agreed', 'own', 'owned', 'tradition', 'traditional', 'sensation', 'sensational'], ['meet', 'meeting', 'plot', 'plotted', 'reference', 'references']]\n"
     ]
    }
   ],
   "source": [
    "print(t26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T10:04:38.631775Z",
     "start_time": "2020-04-06T10:04:38.625227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['caress', 'care', 'fly', 'fly', 'die', 'dy', 'die', 'mule', 'deny', 'deny'], ['agree', 'agree', 'own', 'owned', 'tradition', 'traditional', 'sensation', 'sensational'], ['meet', 'meeting', 'plot', 'plot', 'reference', 'reference']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_text_list(t26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:23:40.350233Z",
     "start_time": "2020-04-06T07:23:40.341394Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextTokenizer():\n",
    "    def __init__(self, texts=None):\n",
    "        self.texts = texts   \n",
    "        self.tokenizer = Tokenizer()\n",
    "        if texts is not None:\n",
    "            self.tokenizer.fit_on_texts(texts)\n",
    "            \n",
    "#         tokenizer.word_counts\n",
    "#         tokenizer.word_docs\n",
    "#         tokenizer.word_index\n",
    "#         tokenizer.document_count\n",
    "        \n",
    "    def train(self, train_text):\n",
    "        self.tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "    def to_sequences(self, in_text=None):\n",
    "        if in_text is None:\n",
    "            return self.tokenizer.texts_to_sequences(self.texts)\n",
    "        else:\n",
    "            return self.tokenizer.texts_to_sequences(in_text)\n",
    "    \n",
    "    def max_length(self):\n",
    "        sequences = self.to_sequences()\n",
    "        lenth = []\n",
    "        for i in sequences:\n",
    "            lenth.append(len(i))\n",
    "        mx_lenth = max(lenth)\n",
    "        \n",
    "        print(lenth.index(mx_lenth), mx_lenth)\n",
    "    \n",
    "    \n",
    "    def max_length2(self):  \n",
    "        sequences = self.to_sequences()\n",
    "        lenth = 0\n",
    "        for i in sequences:\n",
    "            if len(i) > lenth:\n",
    "                lenth = len(i)\n",
    "                idx = sequences.index(i)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        print(idx, lenth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T11:54:07.241685Z",
     "start_time": "2020-04-06T11:52:54.015188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45832 4219\n"
     ]
    }
   ],
   "source": [
    "tk = TextTokenizer(jd_list)\n",
    "tk.max_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T12:17:14.077403Z",
     "start_time": "2020-04-06T12:16:44.158204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45832 4219\n"
     ]
    }
   ],
   "source": [
    "tk.max_length2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:24:01.178186Z",
     "start_time": "2020-04-06T07:24:01.175175Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feature_count(text_list_segmented):\n",
    "    tk = TextTokenizer(text_list_segmented)\n",
    "    feature_count = len(tk.tokenizer.word_counts)\n",
    "    \n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:24:21.308518Z",
     "start_time": "2020-04-06T07:24:21.305919Z"
    }
   },
   "outputs": [],
   "source": [
    "test_jd = ['Job DescriptionImportant Note: During the  application process, ensure your contact information (email and phone number)  is up to date. The invitation can be sent by both email and  text message. In order to receive text message invitations, your profile must  include a mobile phone number designated as “Personal Cell” or “Cellular” in  the contact information of your application.At Wells Fargo, we want to  satisfy our customers’ financial needs and help them succeed financially.  We’re looking for talented people who will put our customers at the center of  everything we do.',\n",
    "           'This role will be based in Charlotte, but will consider other hub  locations.Required Qualifications10 + years of experience in compliance,  operational risk management(includes audit, legal, credit risk, market risk, or the management of a process or business with accountability for compliance or operational risk), or a combination of both; or 10 + years of IT systems  security, business process management or financial services industry  experience, of which 5 + years must include direct experience in compliance, or a combination of bothDesired  QualificationsAdvanced Microsoft Office skillsExcellent verbal, written, and interpersonal communication skillsStrong analytical skills. with high  attention to detail and accuracyAbility to interact, provide feedback/direction',\n",
    "           'Min: $110,600 Mid: $158,000Street AddressNC-Charlotte: 301 S College St -  Charlotte, NCDisclaimerAll offers for employment with Wells Fargo, website: https://www.wellsfargo.com',\n",
    "           'this sentnce has misspelled werds and combinedwords',\n",
    "           'caresses care fly flies die dies died mules deny denied agree agreed own owned tradition traditional sensation sensational meet meeting plot plotted reference references'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test data explanation**  \n",
    "Line 1, 2, 3 are actual jd snippet from database, there are combined words, special characters and websites  \n",
    "Line 4 is for testing spell checker  \n",
    "Line 5 is for testing stemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:25:34.139292Z",
     "start_time": "2020-04-06T07:25:34.135973Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Job DescriptionImportant Note: During the  application process, ensure your contact information (email and phone number)  is up to date. The invitation can be sent by both email and  text message. In order to receive text message invitations, your profile must  include a mobile phone number designated as “Personal Cell” or “Cellular” in  the contact information of your application.At Wells Fargo, we want to  satisfy our customers’ financial needs and help them succeed financially.  We’re looking for talented people who will put our customers at the center of  everything we do.',\n",
       " 'This role will be based in Charlotte, but will consider other hub  locations.Required Qualifications10 + years of experience in compliance,  operational risk management(includes audit, legal, credit risk, market risk, or the management of a process or business with accountability for compliance or operational risk), or a combination of both; or 10 + years of IT systems  security, business process management or financial services industry  experience, of which 5 + years must include direct experience in compliance, or a combination of bothDesired  QualificationsAdvanced Microsoft Office skillsExcellent verbal, written, and interpersonal communication skillsStrong analytical skills. with high  attention to detail and accuracyAbility to interact, provide feedback/direction',\n",
       " 'Min: $110,600 Mid: $158,000Street AddressNC-Charlotte: 301 S College St -  Charlotte, NCDisclaimerAll offers for employment with Wells Fargo, website: https://www.wellsfargo.com',\n",
       " 'this sentnce has misspelled werds and combinedwords',\n",
       " 'caresses care fly flies die dies died mules deny denied agree agreed own owned tradition traditional sensation sensational meet meeting plot plotted reference references']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_jd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 - Word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:25:54.006840Z",
     "start_time": "2020-04-06T07:25:54.002580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job', 'descriptionimportant', 'note', 'during', 'the', 'application', 'process', 'ensure', 'your', 'contact', 'information', 'email', 'and', 'phone', 'number', 'is', 'up', 'to', 'date', 'the', 'invitation', 'can', 'be', 'sent', 'by', 'both', 'email', 'and', 'text', 'message', 'in', 'order', 'to', 'receive', 'text', 'message', 'invitations', 'your', 'profile', 'must', 'include', 'a', 'mobile', 'phone', 'number', 'designated', 'as', '“personal', 'cell”', 'or', '“cellular”', 'in', 'the', 'contact', 'information', 'of', 'your', 'application', 'at', 'wells', 'fargo', 'we', 'want', 'to', 'satisfy', 'our', 'customers’', 'financial', 'needs', 'and', 'help', 'them', 'succeed', 'financially', 'we’re', 'looking', 'for', 'talented', 'people', 'who', 'will', 'put', 'our', 'customers', 'at', 'the', 'center', 'of', 'everything', 'we', 'do']\n",
      "['this', 'role', 'will', 'be', 'based', 'in', 'charlotte', 'but', 'will', 'consider', 'other', 'hub', 'locations', 'required', 'qualifications10', 'years', 'of', 'experience', 'in', 'compliance', 'operational', 'risk', 'management', 'includes', 'audit', 'legal', 'credit', 'risk', 'market', 'risk', 'or', 'the', 'management', 'of', 'a', 'process', 'or', 'business', 'with', 'accountability', 'for', 'compliance', 'or', 'operational', 'risk', 'or', 'a', 'combination', 'of', 'both', 'or', '10', 'years', 'of', 'it', 'systems', 'security', 'business', 'process', 'management', 'or', 'financial', 'services', 'industry', 'experience', 'of', 'which', '5', 'years', 'must', 'include', 'direct', 'experience', 'in', 'compliance', 'or', 'a', 'combination', 'of', 'bothdesired', 'qualificationsadvanced', 'microsoft', 'office', 'skillsexcellent', 'verbal', 'written', 'and', 'interpersonal', 'communication', 'skillsstrong', 'analytical', 'skills', 'with', 'high', 'attention', 'to', 'detail', 'and', 'accuracyability', 'to', 'interact', 'provide', 'feedback', 'direction']\n",
      "['min', '110', '600', 'mid', '158', '000street', 'addressnc', 'charlotte', '301', 's', 'college', 'st', 'charlotte', 'ncdisclaimerall', 'offers', 'for', 'employment', 'with', 'wells', 'fargo', 'website', 'https', 'www', 'wellsfargo', 'com']\n",
      "['this', 'sentnce', 'has', 'misspelled', 'werds', 'and', 'combinedwords']\n",
      "['caresses', 'care', 'fly', 'flies', 'die', 'dies', 'died', 'mules', 'deny', 'denied', 'agree', 'agreed', 'own', 'owned', 'tradition', 'traditional', 'sensation', 'sensational', 'meet', 'meeting', 'plot', 'plotted', 'reference', 'references']\n"
     ]
    }
   ],
   "source": [
    "# Word segmentation\n",
    "test_jd_seg = segment_text_list(test_jd)\n",
    "\n",
    "for seg in test_jd_seg:\n",
    "    print(seg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 - Correct misspelled word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:39:24.917363Z",
     "start_time": "2020-04-06T07:38:59.530290Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job', 'descriptionimportant', 'note', 'during', 'the', 'application', 'process', 'ensure', 'your', 'contact', 'information', 'email', 'and', 'phone', 'number', 'is', 'up', 'to', 'date', 'the', 'invitation', 'can', 'be', 'sent', 'by', 'both', 'email', 'and', 'text', 'message', 'in', 'order', 'to', 'receive', 'text', 'message', 'invitations', 'your', 'profile', 'must', 'include', 'a', 'mobile', 'phone', 'number', 'designated', 'as', 'personal', 'cells', 'or', 'cellular', 'in', 'the', 'contact', 'information', 'of', 'your', 'application', 'at', 'wells', 'fargo', 'we', 'want', 'to', 'satisfy', 'our', 'customers', 'financial', 'needs', 'and', 'help', 'them', 'succeed', 'financially', 'were', 'looking', 'for', 'talented', 'people', 'who', 'will', 'put', 'our', 'customers', 'at', 'the', 'center', 'of', 'everything', 'we', 'do']\n",
      "['this', 'role', 'will', 'be', 'based', 'in', 'charlotte', 'but', 'will', 'consider', 'other', 'hub', 'locations', 'required', 'qualifications', 'years', 'of', 'experience', 'in', 'compliance', 'operational', 'risk', 'management', 'includes', 'audit', 'legal', 'credit', 'risk', 'market', 'risk', 'or', 'the', 'management', 'of', 'a', 'process', 'or', 'business', 'with', 'accountability', 'for', 'compliance', 'or', 'operational', 'risk', 'or', 'a', 'combination', 'of', 'both', 'or', '10', 'years', 'of', 'it', 'systems', 'security', 'business', 'process', 'management', 'or', 'financial', 'services', 'industry', 'experience', 'of', 'which', '5', 'years', 'must', 'include', 'direct', 'experience', 'in', 'compliance', 'or', 'a', 'combination', 'of', 'bothdesired', 'qualificationsadvanced', 'microsoft', 'office', 'skillsexcellent', 'verbal', 'written', 'and', 'interpersonal', 'communication', 'skillsstrong', 'analytical', 'skills', 'with', 'high', 'attention', 'to', 'detail', 'and', 'accuracyability', 'to', 'interact', 'provide', 'feedback', 'direction']\n",
      "['min', '110', '600', 'mid', '158', '000street', 'address', 'charlotte', '301', 's', 'college', 'st', 'charlotte', 'ncdisclaimerall', 'offers', 'for', 'employment', 'with', 'wells', 'fargo', 'webster', 'steps', 'wow', 'wellsfargo', 'com']\n",
      "['this', 'sentence', 'has', 'misspelled', 'words', 'and', 'combinedwords']\n",
      "['caresses', 'care', 'fly', 'flies', 'die', 'dies', 'died', 'mules', 'deny', 'denied', 'agree', 'agreed', 'own', 'owned', 'tradition', 'traditional', 'sensation', 'sensational', 'meet', 'meeting', 'plot', 'plotted', 'reference', 'references']\n"
     ]
    }
   ],
   "source": [
    "# Correct misspelled word\n",
    "\n",
    "cs = CheckSpell()\n",
    "test_jd_seg_cs = cs.spell_correction(test_jd_seg)\n",
    "\n",
    "for seg in test_jd_seg_cs:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T07:27:46.003956Z",
     "start_time": "2020-04-06T07:27:45.998615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Unknown words before correction:\n",
      "['we’re', 'customers’', '“cellular”', 'descriptionimportant', '“personal', 'cell”']\n",
      "['skillsstrong', 'accuracyability', 'qualificationsadvanced', 'bothdesired', 'skillsexcellent', 'qualifications10']\n",
      "['website', 'addressnc', 'wellsfargo', '000street', 'www', 'https', 'ncdisclaimerall']\n",
      "['combinedwords', 'sentnce', 'werds']\n",
      "[]\n",
      "\n",
      "***Unknown words after correction:\n",
      "['descriptionimportant']\n",
      "['skillsstrong', 'accuracyability', 'qualificationsadvanced', 'bothdesired', 'skillsexcellent']\n",
      "['000street', 'wellsfargo', 'ncdisclaimerall']\n",
      "['combinedwords']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# We can see some words are not corrected, combined word cannot be handled\n",
    "\n",
    "print(\"***Unknown words before correction:\" )\n",
    "for word_list in cs.find_unknown(test_jd_seg):\n",
    "    print(word_list)\n",
    "    \n",
    "print(\"\\n***Unknown words after correction:\" )\n",
    "for word_list in cs.find_unknown(test_jd_seg_cs):\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 - Keep pure words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T08:32:25.985021Z",
     "start_time": "2020-04-06T08:32:25.981452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job', 'descriptionimportant', 'note', 'during', 'the', 'application', 'process', 'ensure', 'your', 'contact', 'information', 'email', 'and', 'phone', 'number', 'is', 'up', 'to', 'date', 'the', 'invitation', 'can', 'be', 'sent', 'by', 'both', 'email', 'and', 'text', 'message', 'in', 'order', 'to', 'receive', 'text', 'message', 'invitations', 'your', 'profile', 'must', 'include', 'a', 'mobile', 'phone', 'number', 'designated', 'as', 'personal', 'cells', 'or', 'cellular', 'in', 'the', 'contact', 'information', 'of', 'your', 'application', 'at', 'wells', 'fargo', 'we', 'want', 'to', 'satisfy', 'our', 'customers', 'financial', 'needs', 'and', 'help', 'them', 'succeed', 'financially', 'were', 'looking', 'for', 'talented', 'people', 'who', 'will', 'put', 'our', 'customers', 'at', 'the', 'center', 'of', 'everything', 'we', 'do']\n",
      "['this', 'role', 'will', 'be', 'based', 'in', 'charlotte', 'but', 'will', 'consider', 'other', 'hub', 'locations', 'required', 'qualifications', 'years', 'of', 'experience', 'in', 'compliance', 'operational', 'risk', 'management', 'includes', 'audit', 'legal', 'credit', 'risk', 'market', 'risk', 'or', 'the', 'management', 'of', 'a', 'process', 'or', 'business', 'with', 'accountability', 'for', 'compliance', 'or', 'operational', 'risk', 'or', 'a', 'combination', 'of', 'both', 'or', 'years', 'of', 'it', 'systems', 'security', 'business', 'process', 'management', 'or', 'financial', 'services', 'industry', 'experience', 'of', 'which', 'years', 'must', 'include', 'direct', 'experience', 'in', 'compliance', 'or', 'a', 'combination', 'of', 'bothdesired', 'qualificationsadvanced', 'microsoft', 'office', 'skillsexcellent', 'verbal', 'written', 'and', 'interpersonal', 'communication', 'skillsstrong', 'analytical', 'skills', 'with', 'high', 'attention', 'to', 'detail', 'and', 'accuracyability', 'to', 'interact', 'provide', 'feedback', 'direction']\n",
      "['min', '', 'mid', 'street', 'address', 'charlotte', 's', 'college', 'st', 'charlotte', 'ncdisclaimerall', 'offers', 'for', 'employment', 'with', 'wells', 'fargo', 'webster', 'steps', 'wow', 'wellsfargo', 'com']\n",
      "['this', 'sentence', 'has', 'misspelled', 'words', 'and', 'combinedwords']\n",
      "['caresses', 'care', 'fly', 'flies', 'die', 'dies', 'died', 'mules', 'deny', 'denied', 'agree', 'agreed', 'own', 'owned', 'tradition', 'traditional', 'sensation', 'sensational', 'meet', 'meeting', 'plot', 'plotted', 'reference', 'references']\n"
     ]
    }
   ],
   "source": [
    "test_jd_seg_cs_kp = keep_pure_text(test_jd_seg_cs)\n",
    "#test_jd_seg_cs_kp = keep_pure_text(test_jd_seg_cs) # It is a bug, have to run it twice to get it work\n",
    "\n",
    "for seg in test_jd_seg_cs_kp:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 - Filter Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T10:54:14.962766Z",
     "start_time": "2020-04-06T10:54:14.957258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job', 'descriptionimportant', 'note', 'application', 'process', 'ensure', 'contact', 'email', 'phone', 'invitation', 'email', 'text', 'message', 'receive', 'text', 'message', 'invitations', 'profile', 'mobile', 'phone', 'designated', 'personal', 'cells', 'cellular', 'contact', 'application', 'fargo', 'satisfy', 'customers', 'financial', 'succeed', 'financially', 'talented', 'people', 'customers', 'center']\n",
      "['role', 'based', 'charlotte', 'hub', 'locations', 'required', 'qualifications', 'experience', 'compliance', 'operational', 'risk', 'management', 'includes', 'audit', 'legal', 'credit', 'risk', 'market', 'risk', 'management', 'process', 'business', 'accountability', 'compliance', 'operational', 'risk', 'combination', 'systems', 'security', 'business', 'process', 'management', 'financial', 'services', 'industry', 'experience', 'direct', 'experience', 'compliance', 'combination', 'bothdesired', 'qualificationsadvanced', 'microsoft', 'office', 'skillsexcellent', 'verbal', 'written', 'interpersonal', 'communication', 'skillsstrong', 'analytical', 'skills', 'attention', 'detail', 'accuracyability', 'interact', 'provide', 'feedback', 'direction']\n",
      "['min', '', 'mid', 'street', 'address', 'charlotte', 'college', 'st', 'charlotte', 'ncdisclaimerall', 'offers', 'employment', 'fargo', 'webster', 'steps', 'wow', 'wellsfargo']\n",
      "['sentence', 'misspelled', 'combinedwords']\n",
      "['caresses', 'care', 'fly', 'flies', 'die', 'dies', 'died', 'mules', 'deny', 'denied', 'agree', 'agreed', 'owned', 'tradition', 'traditional', 'sensation', 'sensational', 'meet', 'meeting', 'plot', 'plotted', 'reference', 'references']\n"
     ]
    }
   ],
   "source": [
    "test_jd_seg_cs_kp_fs = filter_stopwords(test_jd_seg_cs_kp, stopwords)\n",
    "\n",
    "for seg in test_jd_seg_cs_kp_fs:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:32.968473Z",
     "start_time": "2020-04-05T12:28:32.960590Z"
    }
   },
   "outputs": [],
   "source": [
    "test_jd_seg_cs_kp_fs_s =  stemer_starter(test_jd_seg_cs_kp_fs, 'Porter')\n",
    "\n",
    "for seg in test_jd_seg_cs_kp_fs_s:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.6 - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T10:54:36.037278Z",
     "start_time": "2020-04-06T10:54:35.980010Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-f6a3b270f725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_jd_seg_cs_kp_fs_l\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_text_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_jd_seg_cs_kp_fs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_jd_seg_cs_kp_fs_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5510e874e540>\u001b[0m in \u001b[0;36mlemmatize_text_list\u001b[0;34m(text_list_segmented)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_text_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list_segmented\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatize_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_list_segmented\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5510e874e540>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_text_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list_segmented\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatize_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_list_segmented\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5510e874e540>\u001b[0m in \u001b[0;36mlemmatize_list\u001b[0;34m(word_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m    161\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eng'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_tagdict\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_tagdict\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'!YEAR'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'!DIGITS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "test_jd_seg_cs_kp_fs_l  = lemmatize_text_list(test_jd_seg_cs_kp_fs)\n",
    "\n",
    "for seg in test_jd_seg_cs_kp_fs_l:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:32.975085Z",
     "start_time": "2020-04-05T12:28:32.970852Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nFeature count before cleaning: \", get_feature_count(test_jd_seg))\n",
    "print(\"Feature count after cleaning: \", get_feature_count(test_jd_seg_cs_kp_fs_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeature count before cleaning: \", get_feature_count(test_jd_seg))\n",
    "print(\"Feature count after cleaning: \", get_feature_count(test_jd_seg_cs_kp_fs_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Test on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:45:27.666089Z",
     "start_time": "2020-04-05T12:45:09.462260Z"
    }
   },
   "outputs": [],
   "source": [
    "jd_seq = segment_text_list(jd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CheckSpell()\n",
    "jd_unknown = cs.find_unknown(jd_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:48:37.734796Z",
     "start_time": "2020-04-05T12:48:37.730759Z"
    }
   },
   "outputs": [],
   "source": [
    "def jd_cleaning(jd_seq):\n",
    "    cs = CheckSpell()\n",
    "    jd_seg_cs = cs.spell_correction(jd_seq)\n",
    "    jd_seg_cs_kp = keep_pure_text(jd_seg_cs)\n",
    "    jd_seg_cs_kp = keep_pure_text(jd_seg_cs) # It is a bug, have to run it twice to get it work\n",
    "    jd_seg_cs_kp_fs = filter_stopwords(jd_seg_cs_kp, stopwords)\n",
    "    jd_seg_cs_kp_fs_s =  stemer_starter(jd_seg_cs_kp_fs, 'Porter')\n",
    "    \n",
    "    return jd_seg_cs_kp_fs_s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jd_cleaning(jd_seg):\n",
    "    cs = CheckSpell()\n",
    "    jd_seg_cs = cs.spell_correction(jd_seg)\n",
    "    jd_seg_cs_kp = keep_pure_text(jd_seg_cs)\n",
    "    jd_seg_cs_kp = keep_pure_text(jd_seg_cs) # It is a bug, have to run it twice to get it work\n",
    "    jd_seg_cs_kp_fs = filter_stopwords(jd_seg_cs_kp, stopwords)\n",
    "    jd_seg_cs_kp_fs_s =  stemer_starter(jd_seg_cs_kp_fs, 'Porter')\n",
    "    \n",
    "    return jd_seg_cs_kp_fs_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T06:46:14.944367Z",
     "start_time": "2020-04-05T12:49:40.565255Z"
    }
   },
   "outputs": [],
   "source": [
    "jd_seq_clened = jd_cleaning(jd_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T06:46:14.969497Z",
     "start_time": "2020-04-05T12:48:42.533Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nFeature count before stem: \", get_feature_count(jd_seg))\n",
    "print(\"Feature count after stem: \", get_feature_count(jd_seq_clened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T06:46:14.986166Z",
     "start_time": "2020-04-05T12:48:44.109Z"
    }
   },
   "outputs": [],
   "source": [
    "test_jd_cleaned = jd_cleaning(test_jd_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T06:46:15.006862Z",
     "start_time": "2020-04-05T12:48:44.661Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nFeature count before stem: \", get_feature_count(test_jd_seg))\n",
    "print(\"Feature count after stem: \", get_feature_count(test_jd_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "196.989px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
