{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:03.945768Z",
     "start_time": "2020-04-05T12:28:01.211664Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.556455Z",
     "start_time": "2020-04-05T12:28:03.948690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/OneDrive/\\xe8\\x96\\xaa\\xe6\\x99\\xba/meta_training_data_jd_0401.csv' does not exist: b'/OneDrive/\\xe8\\x96\\xaa\\xe6\\x99\\xba/meta_training_data_jd_0401.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-034cd81b2981>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mproject_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/Users/apple/Documents/Xinzhi/Data/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/OneDrive/薪智/meta_training_data_jd_0401.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/OneDrive/\\xe8\\x96\\xaa\\xe6\\x99\\xba/meta_training_data_jd_0401.csv' does not exist: b'/OneDrive/\\xe8\\x96\\xaa\\xe6\\x99\\xba/meta_training_data_jd_0401.csv'"
     ]
    }
   ],
   "source": [
    "# Read data from local pathway\n",
    "\n",
    "project_path = \"/Users/apple/Documents/Xinzhi/Data/\"\n",
    "data = pd.read_csv(r'C:\\Users\\Carl\\Documents\\GitHub\\SmartSalary/meta_training_data_jd_0401.csv')\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.563011Z",
     "start_time": "2020-04-05T12:28:05.558723Z"
    }
   },
   "outputs": [],
   "source": [
    "# get stop words\n",
    "\n",
    "def get_stopwords(words_file):\n",
    "    stopwords = []\n",
    "    with open(words_file, 'r') as file:\n",
    "        for word in file.readlines():\n",
    "            stopwords.append(word.strip('\\n'))\n",
    "    \n",
    "    return stopwords\n",
    "\n",
    "\n",
    "stopwords = get_stopwords(\"stop_words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.573515Z",
     "start_time": "2020-04-05T12:28:05.566037Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The number of tier 1 job title: \", len(data.fun1_code.unique()))\n",
    "print(\"The number of tier 2 job title: \", len(data.Code.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.629834Z",
     "start_time": "2020-04-05T12:28:05.575309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class 'Code' that only have 1 instance\n",
    "jd_count = data.groupby('Code').count()\n",
    "jd_count[jd_count['jd']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 -  JD List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.636695Z",
     "start_time": "2020-04-05T12:28:05.632070Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the list of jds\n",
    "jd_list = data.jd.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.642957Z",
     "start_time": "2020-04-05T12:28:05.640308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test base: `text_to_word_sequence`\n",
    "def segment_text_list(text_list):\n",
    "    text_list_segmented = [text_to_word_sequence(text) for text in text_list]\n",
    "    \n",
    "    return text_list_segmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Correct misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.650152Z",
     "start_time": "2020-04-05T12:28:05.645473Z"
    }
   },
   "outputs": [],
   "source": [
    "class CheckSpell():\n",
    "    def __init__(self):\n",
    "        self.spell = SpellChecker()\n",
    "    \n",
    "    def find_unknown(self, text_list_segmented):\n",
    "        unkonwn_words = [list(self.spell.unknown(word_list)) for word_list in text_list_segmented]\n",
    "        return unkonwn_words\n",
    "    \n",
    "    def spell_correction(self, text_list_segmented):\n",
    "        corrected = [[self.spell.correction(word) for word in word_list] for word_list in text_list_segmented]\n",
    "        return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Keep pure words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.658729Z",
     "start_time": "2020-04-05T12:28:05.652978Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean the segmented text list, keep only pure english words        \n",
    "\n",
    "def keep_pure_text(text_list_segmented):\n",
    "    for word_list in text_list_segmented:\n",
    "        for word in word_list:\n",
    "            match = re.findall(r'^[a-z]+$', word)\n",
    "            if match:\n",
    "                pass\n",
    "            else: \n",
    "                word_list.remove(word)\n",
    "    \n",
    "    return text_list_segmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Filter Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.665684Z",
     "start_time": "2020-04-05T12:28:05.662246Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_stopwords(text_list_segmented, stop_words):\n",
    "    filtered_words = [[word for word in word_list if word not in stop_words]\n",
    "                      for word_list in text_list_segmented]\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.672676Z",
     "start_time": "2020-04-05T12:28:05.668304Z"
    }
   },
   "outputs": [],
   "source": [
    "def stemer_starter(text_list_segmented, in_stemmer='Porter'):\n",
    "    if in_stemmer == 'Snowball':\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "    \n",
    "    result = [[stemmer.stem(word) for word in word_list] for word_list in text_list_segmented]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.677614Z",
     "start_time": "2020-04-05T12:28:05.674796Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_stem(text_list_segmented, in_stemmer):\n",
    "    print(\"\\nFeature count before stem: \", get_feature_count(text_list_segmented))\n",
    "    stemmed = stemer_starter(text_list_segmented, in_stemmer)\n",
    "    print(\"Feature count after stem: \", get_feature_count(stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.681960Z",
     "start_time": "2020-04-05T12:28:05.679474Z"
    }
   },
   "outputs": [],
   "source": [
    "# stem_test_text = [['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "#                    'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "#                    'meeting', 'stating', 'siezing', 'itemization',\n",
    "#                    'sensational', 'traditional', 'reference', 'colonizer',\n",
    "#                    'plotted'],\n",
    "#                   ['try', 'tries', 'tring', 'apple', 'apples', 'watch', 'watches',\n",
    "#                    'teeth', 'tooth', 'foot', 'feet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.686512Z",
     "start_time": "2020-04-05T12:28:05.684453Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"***Porter Stemmer: \")\n",
    "# print(stemer_starter(stem_test_text, 'Porter'))\n",
    "# test_stem(stem_test_text, 'Porter')\n",
    "\n",
    "# print(\"\\n***Snowball Stemmer: \")\n",
    "# print(stemer_starter(stem_test_text, 'Porter'))\n",
    "# test_stem(stem_test_text, 'Porter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.699091Z",
     "start_time": "2020-04-05T12:28:05.688400Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextTokenizer():\n",
    "    def __init__(self, texts=None):\n",
    "        self.texts = texts   \n",
    "        self.tokenizer = Tokenizer()\n",
    "        if texts is not None:\n",
    "            self.tokenizer.fit_on_texts(texts)\n",
    "            \n",
    "#         tokenizer.word_counts\n",
    "#         tokenizer.word_docs\n",
    "#         tokenizer.word_index\n",
    "#         tokenizer.document_count\n",
    "        \n",
    "    def train(self, train_text):\n",
    "        self.tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "    def to_sequences(self, in_text=None):\n",
    "        if in_text is None:\n",
    "            return self.tokenizer.texts_to_sequences(self.texts)\n",
    "        else:\n",
    "            return self.tokenizer.texts_to_sequences(in_text)\n",
    "    \n",
    "    def max_length(self):\n",
    "        sequences = self.to_sequences()\n",
    "        lenth = []\n",
    "        for i in sequences:\n",
    "            lenth.append(len(i))\n",
    "        mx_lenth = max(lenth)\n",
    "        \n",
    "        print(lenth.index(mx_lenth), mx_lenth)\n",
    "    \n",
    "    \n",
    "    def max_length2(self):  \n",
    "        sequences = self.to_sequences()\n",
    "        lenth = 0\n",
    "        for i in sequences:\n",
    "            if len(i) > lenth:\n",
    "                lenth = len(i)\n",
    "                idx = sequences.index(i)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        print(idx, lenth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.706367Z",
     "start_time": "2020-04-05T12:28:05.701962Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feature_count(text_list_segmented):\n",
    "    tk = TextTokenizer(text_list_segmented)\n",
    "    feature_count = len(tk.tokenizer.word_counts)\n",
    "    \n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.714175Z",
     "start_time": "2020-04-05T12:28:05.710301Z"
    }
   },
   "outputs": [],
   "source": [
    "test_jd = ['Job DescriptionImportant Note: During the  application process, ensure your contact information (email and phone number)  is up to date. The invitation can be sent by both email and  text message. In order to receive text message invitations, your profile must  include a mobile phone number designated as “Personal Cell” or “Cellular” in  the contact information of your application.At Wells Fargo, we want to  satisfy our customers’ financial needs and help them succeed financially.  We’re looking for talented people who will put our customers at the center of  everything we do.',\n",
    "           'This role will be based in Charlotte, but will consider other hub  locations.Required Qualifications10 + years of experience in compliance,  operational risk management(includes audit, legal, credit risk, market risk, or the management of a process or business with accountability for compliance or operational risk), or a combination of both; or 10 + years of IT systems  security, business process management or financial services industry  experience, of which 5 + years must include direct experience in compliance, or a combination of bothDesired  QualificationsAdvanced Microsoft Office skillsExcellent verbal, written, and interpersonal communication skillsStrong analytical skills. with high  attention to detail and accuracyAbility to interact, provide feedback/direction',\n",
    "           'Min: $110,600 Mid: $158,000Street AddressNC-Charlotte: 301 S College St -  Charlotte, NCDisclaimerAll offers for employment with Wells Fargo, website: https://www.wellsfargo.com',\n",
    "           'this sentnce has misspelled werds and combinedwords',\n",
    "           'caresses care fly flies die dies died mules deny denied agree agreed own owned tradition traditional sensation sensational meet meeting plot plotted reference references'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test data explanation**  \n",
    "Line 1, 2, 3 are actual jd snippet from database, there are combined words, special characters and websites  \n",
    "Line 4 is for testing spell checker  \n",
    "Line 5 is for testing stemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.720978Z",
     "start_time": "2020-04-05T12:28:05.716708Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_jd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 - Word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.727395Z",
     "start_time": "2020-04-05T12:28:05.723042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word segmentation\n",
    "test_jd_seg = segment_text_list(test_jd)\n",
    "\n",
    "for seg in test_jd_seg:\n",
    "    print(seg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 - Correct misspelled word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:05.866400Z",
     "start_time": "2020-04-05T12:28:05.729913Z"
    }
   },
   "outputs": [],
   "source": [
    "cs = CheckSpell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:32.926002Z",
     "start_time": "2020-04-05T12:28:05.868330Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Correct misspelled word\n",
    "\n",
    "test_jd_seg_cs = cs.spell_correction(test_jd_seg)\n",
    "\n",
    "for seg in test_jd_seg_cs:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:32.939496Z",
     "start_time": "2020-04-05T12:28:32.933540Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can see some words are not corrected, combined word cannot be handled\n",
    "\n",
    "print(\"***Unknown words before correction:\" )\n",
    "for word_list in cs.find_unknown(test_jd_seg):\n",
    "    print(word_list)\n",
    "    \n",
    "print(\"\\n***Unknown words after correction:\" )\n",
    "for word_list in cs.find_unknown(test_jd_seg_cs):\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 - Keep pure words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:32.948665Z",
     "start_time": "2020-04-05T12:28:32.944175Z"
    }
   },
   "outputs": [],
   "source": [
    "test_jd_seg_cs_kp = keep_pure_text(test_jd_seg_cs)\n",
    "test_jd_seg_cs_kp = keep_pure_text(test_jd_seg_cs) # It is a bug, have to run it twice to get it work\n",
    "\n",
    "\n",
    "for seg in test_jd_seg_cs_kp:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 - Filter Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:32.958026Z",
     "start_time": "2020-04-05T12:28:32.951101Z"
    }
   },
   "outputs": [],
   "source": [
    "test_jd_seg_cs_kp_fs = filter_stopwords(test_jd_seg_cs_kp, stopwords)\n",
    "\n",
    "for seg in test_jd_seg_cs_kp_fs:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:32.968473Z",
     "start_time": "2020-04-05T12:28:32.960590Z"
    }
   },
   "outputs": [],
   "source": [
    "test_jd_seg_cs_kp_fs_s =  stemer_starter(test_jd_seg_cs_kp_fs, 'Porter')\n",
    "\n",
    "for seg in test_jd_seg_cs_kp_fs_s:\n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T12:28:32.975085Z",
     "start_time": "2020-04-05T12:28:32.970852Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nFeature count before stem: \", get_feature_count(test_jd_seg))\n",
    "print(\"Feature count after stem: \", get_feature_count(test_jd_seg_cs_kp_fs_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Test on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "196.989px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
